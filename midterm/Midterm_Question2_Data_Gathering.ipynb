{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering for Question 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching data and creating folder structure where data is dtored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import json, glob, os, re, requests, time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NYTimes developer console API key\n",
    "nyt_archive_key = os.getenv(\"nyt_key_1\")\n",
    "# print(nyt_archive_key)\n",
    "\n",
    "# nyt_archive_key = os.getenv(\"nyt_key_2\")\n",
    "# print(nyt_archive_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two different apis to fetch data from added in list\n",
    "apis = ['articlesearch', 'archive']\n",
    "data_folders_directory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions Definitions:\n",
    "## Function to create folder structure to store data for different apis in json format for question 2\n",
    "def create_directory_for_data(api):\n",
    "    current_dir = os.path.dirname('__file__')                                #Relative path for current directory\n",
    "    data_folder = current_dir\n",
    "    return create_subfolders_for_data(data_folder, 'data', 'question2', api) \n",
    "\n",
    "## Function to create subfolder as per the path specified and api names\n",
    "def create_subfolders_for_data(data_folder, data, question, api):\n",
    "    directory =os.path.join(data_folder, data, question, api)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return directory\n",
    "    else:\n",
    "        return directory\n",
    "    \n",
    "#Function to write data to json file at respective location\n",
    "def write_to_json_file(file_path, json_data):\n",
    "    with open(file_path, 'w') as json_out:\n",
    "        json.dump(json_data, json_out, indent=2)\n",
    "\n",
    "## Creates a list containing folder paths for both apis\n",
    "for api in apis:\n",
    "    data_folder_dir = create_directory_for_data(api)\n",
    "    data_folders_directory.append(data_folder_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to get the last 6 months with year and month.\n",
    "# Fetching data for 6 previous months \n",
    "def get_year_and_month_range_for_archives():    \n",
    "    year_count = 0\n",
    "    range_of_years = []\n",
    "    mon = 0\n",
    "    while(year_count < 2):\n",
    "        row = ''\n",
    "        \n",
    "        # Get year in consideration\n",
    "        year_to_consider = str(date.today().year - year_count)\n",
    "        row = str(year_to_consider)\n",
    "        month_count = date.today().month\n",
    "        # If year is not current year, reset month count to 12 to trace Dec - Jan\n",
    "        if(year_count>0):\n",
    "            month_count = 12\n",
    "        \n",
    "        # If month is before January, change the year\n",
    "        while(month_count>0):\n",
    "            if year_count==1 and mon > 5:         # Limiting the search for 6 months thats why 0-5\n",
    "                break\n",
    "            row = str(year_to_consider)+ ','+ str(month_count)\n",
    "            range_of_years.append(row)\n",
    "            mon +=1\n",
    "            month_count -= 1\n",
    "        year_count +=1\n",
    "    return range_of_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to fetch response from api provided\n",
    "## As both apis have different calling parameters, using if to differentiate between the calls\n",
    "def fetch_response_from_api(page_count, api, year, month):\n",
    "    if api == 'articlesearch':\n",
    "        \n",
    "        #URL to hit\n",
    "        url = 'https://api.nytimes.com/svc/search/v2/'+api+'.json'\n",
    "        \n",
    "        #Parameters to pass\n",
    "        payload = {'api-key': nyt_archive_key, 'page': page_count}\n",
    "        response = requests.get(url,params=payload)\n",
    "        return response\n",
    "    elif api == 'archive':\n",
    "        \n",
    "        #URL to hit\n",
    "        url = 'https://api.nytimes.com/svc/'+api+'/v1/'+year+'/'+month+'.json'\n",
    "        \n",
    "        #Parameters to pass\n",
    "        payload = {'api-key': nyt_archive_key}\n",
    "        response = requests.get(url,params=payload)\n",
    "        return response \n",
    "\n",
    "#Returns response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to extract only articles from the response file\n",
    "## Also checking if the article is already present in file or not. If article is present already, don't append it to list of articles\n",
    "def process_response_from_service(response, api, file_path, already_present_file, page_count):\n",
    "    if response.status_code == 200:\n",
    "        res = response.json()\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path) as fil:\n",
    "                \n",
    "                # Get already present file and its content as we will use this multiple times to gather data\n",
    "                already_present_file = json.load(fil)\n",
    "                \n",
    "                # Remove duplicates\n",
    "                this_response = [artic for artic in res['response']['docs'] if artic['_id'] not in [articles['_id'] for articles in already_present_file]]\n",
    "                already_present_file.extend(this_response)\n",
    "        else:\n",
    "            # If the file is not present, dont check for duplicates. Just write the articles into variable\n",
    "            already_present_file.extend(res['response']['docs'])\n",
    "        \n",
    "        #Write output to json format\n",
    "        write_to_json_file(file_path, already_present_file)\n",
    "    else:\n",
    "        #Show error messages in case an API fails\n",
    "        if api == 'articlesearch':\n",
    "            print('Request Failed for article search with status code',response.status_code, 'at Page', page_count)\n",
    "        elif api == 'archive':\n",
    "            print('Request Failed for archives with status code',response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API to hit ->  articlesearch\n",
      "Request Failed for article search with status code 400 at Page 121\n",
      "API to hit ->  archive\n"
     ]
    }
   ],
   "source": [
    "for data_folder_dir in data_folders_directory:\n",
    "    page_count = 0\n",
    "    status_from_json = 200\n",
    "    \n",
    "    # Get the name of api from path\n",
    "    api = data_folder_dir[15:]\n",
    "    print('API to hit -> ',api)\n",
    "    \n",
    "    # Create the name of json file with folder path\n",
    "    file_name = api+'_response_pages'\n",
    "    file_path = os.path.join(data_folder_dir, file_name)\n",
    "    file_path+='.json'\n",
    "\n",
    "    already_present_file = []\n",
    "\n",
    "    # Fetch data till we get error from response \n",
    "    while(status_from_json == 200):  \n",
    "        \n",
    "        # Add time delay between 2 api calls to fetch response without interruption\n",
    "        time.sleep(1)\n",
    "        if api == 'archive':\n",
    "            \n",
    "            #Get year and month range for past 6 months\n",
    "            year_range_for_archive = get_year_and_month_range_for_archives()\n",
    "            for time_to_consider in year_range_for_archive:\n",
    "                # year and month is used but not page count\n",
    "                year = time_to_consider[:4]\n",
    "                month = time_to_consider[5:]\n",
    "                \n",
    "                # Fetch response for each year and month for past 6 months\n",
    "                response = fetch_response_from_api(page_count, api, year, month)\n",
    "                \n",
    "                # save the articles only from response into json file \n",
    "                process_response_from_service(response, api, file_path, already_present_file, page_count)\n",
    "                status_from_json = response.status_code\n",
    "                if status_from_json != 200:\n",
    "                    break\n",
    "            status_from_json = 404\n",
    "        elif api == 'articlesearch':\n",
    "            # year and month are not used but page count is\n",
    "            year = 0\n",
    "            month = 0\n",
    "            \n",
    "            # Fetch response for each page\n",
    "            response = fetch_response_from_api(page_count, api, year, month)\n",
    "            \n",
    "            # save the articles only from response into json file \n",
    "            process_response_from_service(response, api, file_path, already_present_file, page_count)\n",
    "            if response.status_code == 200:\n",
    "                page_count += 1\n",
    "            status_from_json = response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API -  articlesearch\n",
      "Data Count -  3028\n",
      "API -  archive\n",
      "Data Count -  27260\n"
     ]
    }
   ],
   "source": [
    "# Code to check how much data is present in each file.\n",
    "#### No need to run. It will just print the number of articles in each response\n",
    "for data_folder_dir in data_folders_directory:\n",
    "    api = data_folder_dir[15:]\n",
    "    print('API - ',api)\n",
    "    \n",
    "    file_name = api+'_response_pages'\n",
    "    file_path = os.path.join(data_folder_dir, file_name)\n",
    "    file_path+='.json'\n",
    "    with open(file_path) as file_to_read:\n",
    "        present = json.load(file_to_read)\n",
    "        print('Data Count - ', len(present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
